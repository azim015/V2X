{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f571ea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Two Seq2Seq Experiments on RSU Time-Series Parquet\n",
    "-------------------------------------------------\n",
    "Reads:  data/processed/rsu_net_timeseries.parquet\n",
    "\n",
    "Task: multi-horizon RSU load forecasting (Tout steps ahead) from Tin history.\n",
    "\n",
    "Exp-1 (Minimal): ONLY time + RSU aggregated load history\n",
    "    - Inputs: rsu_id embedding + time features + past rsu_load_kbps\n",
    "    - No other network/comm features.\n",
    "\n",
    "Exp-2 (Full): time + RSU load history + ALL aggregated network/comm features\n",
    "    - Inputs: rsu_id embedding + time features + past rsu_load_kbps + feature vector.\n",
    "\n",
    "Training:\n",
    "    - 400 epochs (no early stopping, but best checkpoint is saved)\n",
    "    - Metrics per epoch: MAE, RMSE, R2 (averaged over horizons)\n",
    "    - Plots saved for each experiment\n",
    "\n",
    "Outputs:\n",
    "    results/seq2seq_minimal_history.csv\n",
    "    results/seq2seq_full_history.csv\n",
    "    results/seq2seq_minimal_curves.png\n",
    "    results/seq2seq_full_curves.png\n",
    "    results/seq2seq_minimal_best.pt\n",
    "    results/seq2seq_full_best.pt\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2f0f01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "PARQ_PATH = Path(\"data/processed/rsu_net_timeseries.parquet\")\n",
    "OUT_DIR = Path(\"results\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "Tin = 20\n",
    "Tout = 10\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 400\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "HIDDEN = 128\n",
    "LAYERS = 2\n",
    "DROPOUT = 0.15\n",
    "TEACHER_FORCING = 0.5\n",
    "\n",
    "TRAIN_RATIO = 0.7\n",
    "VAL_RATIO = 0.1\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "484b1634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Utils: Metrics (NumPy)\n",
    "# =========================\n",
    "def multi_horizon_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> dict:\n",
    "    \"\"\"\n",
    "    y_true, y_pred: [N, Tout, 1] in ORIGINAL scale\n",
    "    Returns avg MAE, avg RMSE, avg R2 across horizons.\n",
    "    R2 is computed per-horizon then averaged (stable & interpretable).\n",
    "    \"\"\"\n",
    "    eps = 1e-12\n",
    "    maes, rmses, r2s = [], [], []\n",
    "    for k in range(y_true.shape[1]):\n",
    "        yt = y_true[:, k, 0]\n",
    "        yp = y_pred[:, k, 0]\n",
    "        err = yt - yp\n",
    "        mae = float(np.mean(np.abs(err)))\n",
    "        rmse = float(np.sqrt(np.mean(err ** 2)))\n",
    "\n",
    "        ss_res = float(np.sum((yt - yp) ** 2))\n",
    "        ss_tot = float(np.sum((yt - np.mean(yt)) ** 2)) + eps\n",
    "        r2 = 1.0 - ss_res / ss_tot\n",
    "\n",
    "        maes.append(mae); rmses.append(rmse); r2s.append(r2)\n",
    "    return {\n",
    "        \"mae\": float(np.mean(maes)),\n",
    "        \"rmse\": float(np.mean(rmses)),\n",
    "        \"r2\": float(np.mean(r2s)),\n",
    "        \"mae_per_h\": maes,\n",
    "        \"rmse_per_h\": rmses,\n",
    "        \"r2_per_h\": r2s,\n",
    "    }\n",
    "\n",
    "\n",
    "def add_time_features(df: pd.DataFrame, ts_col: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds general time features without assuming timestamp is datetime.\n",
    "    - If datetime: use hour/day-of-week.\n",
    "    - Else numeric: use sin/cos of normalized index within each rsu_id.\n",
    "    \"\"\"\n",
    "    if np.issubdtype(df[ts_col].dtype, np.datetime64):\n",
    "        dt = pd.to_datetime(df[ts_col])\n",
    "        df[\"hour\"] = dt.dt.hour.astype(np.int16)\n",
    "        df[\"dow\"] = dt.dt.dayofweek.astype(np.int16)\n",
    "        df[\"minute\"] = dt.dt.minute.astype(np.int16)\n",
    "\n",
    "        # cyclical encodings\n",
    "        df[\"hour_sin\"] = np.sin(2 * np.pi * df[\"hour\"] / 24.0)\n",
    "        df[\"hour_cos\"] = np.cos(2 * np.pi * df[\"hour\"] / 24.0)\n",
    "        df[\"dow_sin\"] = np.sin(2 * np.pi * df[\"dow\"] / 7.0)\n",
    "        df[\"dow_cos\"] = np.cos(2 * np.pi * df[\"dow\"] / 7.0)\n",
    "        df[\"min_sin\"] = np.sin(2 * np.pi * df[\"minute\"] / 60.0)\n",
    "        df[\"min_cos\"] = np.cos(2 * np.pi * df[\"minute\"] / 60.0)\n",
    "        return df\n",
    "\n",
    "    # numeric timestamps: create per-RSU normalized \"progress\" feature\n",
    "    # (works even if timestamps are irregular)\n",
    "    df[\"_t_rank\"] = df.groupby(\"rsu_id\")[ts_col].rank(method=\"dense\").astype(np.float32)\n",
    "    df[\"_t_max\"] = df.groupby(\"rsu_id\")[\"_t_rank\"].transform(\"max\").astype(np.float32)\n",
    "    prog = (df[\"_t_rank\"] - 1.0) / (df[\"_t_max\"] - 1.0 + 1e-6)\n",
    "    df[\"t_sin\"] = np.sin(2 * np.pi * prog)\n",
    "    df[\"t_cos\"] = np.cos(2 * np.pi * prog)\n",
    "    df = df.drop(columns=[\"_t_rank\", \"_t_max\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af1f52a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Windowing\n",
    "# =========================\n",
    "def build_windows(\n",
    "    df: pd.DataFrame,\n",
    "    ts_col: str,\n",
    "    rsu_id_col: str,\n",
    "    y_col: str,\n",
    "    x_cols: list[str],\n",
    "    Tin: int,\n",
    "    Tout: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Build windows per RSU:\n",
    "      X: [N, Tin, F]\n",
    "      y: [N, Tout, 1]\n",
    "      rsu_idx: [N] integer RSU index for embedding\n",
    "      anchor: [N] timestamp (end of input window)\n",
    "    \"\"\"\n",
    "    Xs, Ys, Ridx, Anch = [], [], [], []\n",
    "\n",
    "    # map rsu_id to index\n",
    "    rsu_ids = df[rsu_id_col].astype(\"string\").unique().tolist()\n",
    "    rsu2i = {rid: i for i, rid in enumerate(rsu_ids)}\n",
    "\n",
    "    for rid, dfr in df.groupby(rsu_id_col, sort=False):\n",
    "        dfr = dfr.sort_values(ts_col).reset_index(drop=True)\n",
    "        Xmat = dfr[x_cols].to_numpy(dtype=np.float32)\n",
    "        yvec = dfr[y_col].to_numpy(dtype=np.float32)\n",
    "        T = len(dfr)\n",
    "        n = T - Tin - Tout + 1\n",
    "        if n <= 0:\n",
    "            continue\n",
    "\n",
    "        X = np.zeros((n, Tin, len(x_cols)), dtype=np.float32)\n",
    "        y = np.zeros((n, Tout, 1), dtype=np.float32)\n",
    "        a = np.zeros((n,), dtype=np.float64)\n",
    "        r = np.full((n,), rsu2i[str(rid)], dtype=np.int64)\n",
    "\n",
    "        for i in range(n):\n",
    "            in_end = i + Tin\n",
    "            out_end = in_end + Tout\n",
    "            X[i] = Xmat[i:in_end]\n",
    "            y[i, :, 0] = yvec[in_end:out_end]\n",
    "            a[i] = dfr.loc[in_end - 1, ts_col].value if np.issubdtype(dfr[ts_col].dtype, np.datetime64) else float(dfr.loc[in_end - 1, ts_col])\n",
    "\n",
    "        Xs.append(X); Ys.append(y); Ridx.append(r); Anch.append(a)\n",
    "\n",
    "    X_all = np.concatenate(Xs, axis=0)\n",
    "    y_all = np.concatenate(Ys, axis=0)\n",
    "    r_all = np.concatenate(Ridx, axis=0)\n",
    "    a_all = np.concatenate(Anch, axis=0)\n",
    "\n",
    "    # strict time split by anchor\n",
    "    order = np.argsort(a_all)\n",
    "    return X_all[order], y_all[order], r_all[order], rsu2i\n",
    "\n",
    "# =========================\n",
    "# Dataset / scaling\n",
    "# =========================\n",
    "class RSUSeqDataset(Dataset):\n",
    "    def __init__(self, X, y, ridx):\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "        self.ridx = torch.from_numpy(ridx).long()\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, i): return self.X[i], self.y[i], self.ridx[i]\n",
    "\n",
    "\n",
    "def standardize_fit(X_train: np.ndarray):\n",
    "    # X: [N, Tin, F]\n",
    "    mu = X_train.reshape(-1, X_train.shape[-1]).mean(axis=0)\n",
    "    sig = X_train.reshape(-1, X_train.shape[-1]).std(axis=0) + 1e-6\n",
    "    return mu, sig\n",
    "\n",
    "def standardize_apply(X: np.ndarray, mu, sig):\n",
    "    return (X - mu) / sig\n",
    "\n",
    "def scale_y_fit(y_train: np.ndarray):\n",
    "    mu = y_train.reshape(-1).mean()\n",
    "    sig = y_train.reshape(-1).std() + 1e-6\n",
    "    return mu, sig\n",
    "\n",
    "def scale_y_apply(y: np.ndarray, mu, sig):\n",
    "    return (y - mu) / sig\n",
    "\n",
    "def unscale_y(y: np.ndarray, mu, sig):\n",
    "    return y * sig + mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81046e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp column: timestamp\n",
      "Minimal X cols: 3 -> ['rsu_load_kbps', 't_sin', 't_cos']\n",
      "Full X cols:    32 (includes all numeric aggregates)\n",
      "\n",
      "====================\n",
      "Experiment: seq2seq_minimal\n",
      "====================\n",
      "Windows: X=(878401, 20, 3), y=(878401, 10, 1), features=3, RSUs=13\n",
      "Split: train=614880 val=87840 test=175681\n",
      "Epoch 001 | train(MAE=8.103, RMSE=20.072, R2=0.995) | val(MAE=4.327, RMSE=7.321, R2=0.993)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 265\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull X cols:    \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(x_cols_full)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (includes all numeric aggregates)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# Run experiments\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m m1 \u001b[38;5;241m=\u001b[39m run_experiment(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq2seq_minimal\u001b[39m\u001b[38;5;124m\"\u001b[39m, df, ts_col, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrsu_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrsu_load_kbps\u001b[39m\u001b[38;5;124m\"\u001b[39m, x_cols_minimal)\n\u001b[0;32m    266\u001b[0m m2 \u001b[38;5;241m=\u001b[39m run_experiment(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq2seq_full\u001b[39m\u001b[38;5;124m\"\u001b[39m, df, ts_col, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrsu_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrsu_load_kbps\u001b[39m\u001b[38;5;124m\"\u001b[39m, x_cols_full)\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSummary (TEST best checkpoints)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 134\u001b[0m, in \u001b[0;36mrun_experiment\u001b[1;34m(name, df, ts_col, rsu_id_col, y_col, x_cols)\u001b[0m\n\u001b[0;32m    132\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m    133\u001b[0m     opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 134\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# Compute metrics on train/val (original scale)\u001b[39;00m\n\u001b[0;32m    137\u001b[0m ytr_true, ytr_pred \u001b[38;5;241m=\u001b[39m eval_loader(train_loader)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Seq2Seq model with RSU embedding\n",
    "# =========================\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden, layers, dropout):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(in_dim, hidden, num_layers=layers, batch_first=True,\n",
    "                          dropout=dropout if layers > 1 else 0.0)\n",
    "    def forward(self, x):\n",
    "        _, h = self.gru(x)\n",
    "        return h\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden, layers, dropout):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(1, hidden, num_layers=layers, batch_first=True,\n",
    "                          dropout=dropout if layers > 1 else 0.0)\n",
    "        self.fc = nn.Linear(hidden, 1)\n",
    "    def forward(self, y_prev, h):\n",
    "        out, h = self.gru(y_prev, h)\n",
    "        y_hat = self.fc(out)\n",
    "        return y_hat, h\n",
    "\n",
    "class Seq2SeqRSU(nn.Module):\n",
    "    def __init__(self, in_dim, hidden, layers, dropout, n_rsu, emb_dim=16):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(n_rsu, emb_dim)\n",
    "        self.enc = Encoder(in_dim + emb_dim, hidden, layers, dropout)\n",
    "        self.dec = Decoder(hidden, layers, dropout)\n",
    "\n",
    "    def forward(self, x, ridx, y_true=None, teacher_forcing=0.5):\n",
    "        # x: [B, Tin, F]\n",
    "        # ridx: [B]\n",
    "        B = x.size(0)\n",
    "        e = self.emb(ridx).unsqueeze(1).expand(-1, x.size(1), -1)  # [B, Tin, emb]\n",
    "        x_in = torch.cat([x, e], dim=-1)\n",
    "        h = self.enc(x_in)\n",
    "\n",
    "        # IMPORTANT: init decoder with last observed load (assumed feature 0 is load)\n",
    "        # x[:, -1, 0] is scaled load at last input time-step\n",
    "        y_prev = x[:, -1:, 0:1]  # [B, 1, 1]\n",
    "\n",
    "        outs = []\n",
    "        for t in range(Tout):\n",
    "            y_hat, h = self.dec(y_prev, h)\n",
    "            outs.append(y_hat)\n",
    "            if (y_true is not None) and (torch.rand(1).item() < teacher_forcing):\n",
    "                y_prev = y_true[:, t:t+1, :]\n",
    "            else:\n",
    "                y_prev = y_hat\n",
    "        return torch.cat(outs, dim=1)  # [B, Tout, 1]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Training loop (400 epochs)\n",
    "# =========================\n",
    "def run_experiment(\n",
    "    name: str,\n",
    "    df: pd.DataFrame,\n",
    "    ts_col: str,\n",
    "    rsu_id_col: str,\n",
    "    y_col: str,\n",
    "    x_cols: list[str],\n",
    "):\n",
    "    print(f\"\\n====================\\nExperiment: {name}\\n====================\")\n",
    "\n",
    "    X, y, ridx, rsu2i = build_windows(df, ts_col, rsu_id_col, y_col, x_cols, Tin, Tout)\n",
    "    N = len(X)\n",
    "    n_train = int(N * TRAIN_RATIO)\n",
    "    n_val = int(N * VAL_RATIO)\n",
    "\n",
    "    X_train, y_train, r_train = X[:n_train], y[:n_train], ridx[:n_train]\n",
    "    X_val, y_val, r_val = X[n_train:n_train+n_val], y[n_train:n_train+n_val], ridx[n_train:n_train+n_val]\n",
    "    X_test, y_test, r_test = X[n_train+n_val:], y[n_train+n_val:], ridx[n_train+n_val:]\n",
    "\n",
    "    print(f\"Windows: X={X.shape}, y={y.shape}, features={X.shape[-1]}, RSUs={len(rsu2i)}\")\n",
    "    print(f\"Split: train={len(X_train)} val={len(X_val)} test={len(X_test)}\")\n",
    "\n",
    "    # Fit scalers on train only\n",
    "    x_mu, x_sig = standardize_fit(X_train)\n",
    "    y_mu, y_sig = scale_y_fit(y_train)\n",
    "\n",
    "    X_train_s = standardize_apply(X_train, x_mu, x_sig)\n",
    "    X_val_s   = standardize_apply(X_val, x_mu, x_sig)\n",
    "    X_test_s  = standardize_apply(X_test, x_mu, x_sig)\n",
    "\n",
    "    y_train_s = scale_y_apply(y_train, y_mu, y_sig)\n",
    "    y_val_s   = scale_y_apply(y_val, y_mu, y_sig)\n",
    "    y_test_s  = scale_y_apply(y_test, y_mu, y_sig)\n",
    "\n",
    "    train_loader = DataLoader(RSUSeqDataset(X_train_s, y_train_s, r_train), batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "    val_loader   = DataLoader(RSUSeqDataset(X_val_s, y_val_s, r_val), batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader  = DataLoader(RSUSeqDataset(X_test_s, y_test_s, r_test), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = Seq2SeqRSU(in_dim=X.shape[-1], hidden=HIDDEN, layers=LAYERS, dropout=DROPOUT, n_rsu=len(rsu2i)).to(DEVICE)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    loss_fn = nn.SmoothL1Loss(beta=1.0)  # robust to spikes\n",
    "\n",
    "    history = []\n",
    "    best_val = float(\"inf\")\n",
    "\n",
    "    def eval_loader(loader):\n",
    "        model.eval()\n",
    "        ys, yps = [], []\n",
    "        with torch.no_grad():\n",
    "            for Xb, yb, rb in loader:\n",
    "                Xb = Xb.to(DEVICE)\n",
    "                yb = yb.to(DEVICE)\n",
    "                rb = rb.to(DEVICE)\n",
    "                yp = model(Xb, rb, y_true=None, teacher_forcing=0.0)\n",
    "                ys.append(yb.cpu().numpy())\n",
    "                yps.append(yp.cpu().numpy())\n",
    "        y_true_s = np.concatenate(ys, axis=0)\n",
    "        y_pred_s = np.concatenate(yps, axis=0)\n",
    "        # unscale to original\n",
    "        y_true = unscale_y(y_true_s, y_mu, y_sig)\n",
    "        y_pred = unscale_y(y_pred_s, y_mu, y_sig)\n",
    "        return y_true, y_pred\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        for Xb, yb, rb in train_loader:\n",
    "            Xb = Xb.to(DEVICE)\n",
    "            yb = yb.to(DEVICE)\n",
    "            rb = rb.to(DEVICE)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            yp = model(Xb, rb, y_true=yb, teacher_forcing=TEACHER_FORCING)\n",
    "            loss = loss_fn(yp, yb)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        # Compute metrics on train/val (original scale)\n",
    "        ytr_true, ytr_pred = eval_loader(train_loader)\n",
    "        yv_true, yv_pred = eval_loader(val_loader)\n",
    "\n",
    "        m_tr = multi_horizon_metrics(ytr_true, ytr_pred)\n",
    "        m_va = multi_horizon_metrics(yv_true, yv_pred)\n",
    "\n",
    "        # choose best checkpoint by val RMSE (or val MAE)\n",
    "        if m_va[\"rmse\"] < best_val:\n",
    "            best_val = m_va[\"rmse\"]\n",
    "            torch.save(model.state_dict(), OUT_DIR / f\"{name}_best.pt\")\n",
    "\n",
    "        row = {\n",
    "            \"epoch\": epoch,\n",
    "            \"train_mae\": m_tr[\"mae\"],\n",
    "            \"train_rmse\": m_tr[\"rmse\"],\n",
    "            \"train_r2\": m_tr[\"r2\"],\n",
    "            \"val_mae\": m_va[\"mae\"],\n",
    "            \"val_rmse\": m_va[\"rmse\"],\n",
    "            \"val_r2\": m_va[\"r2\"],\n",
    "        }\n",
    "        history.append(row)\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch:03d} | \"\n",
    "                f\"train(MAE={row['train_mae']:.3f}, RMSE={row['train_rmse']:.3f}, R2={row['train_r2']:.3f}) | \"\n",
    "                f\"val(MAE={row['val_mae']:.3f}, RMSE={row['val_rmse']:.3f}, R2={row['val_r2']:.3f})\"\n",
    "            )\n",
    "\n",
    "    hist = pd.DataFrame(history)\n",
    "    hist.to_csv(OUT_DIR / f\"{name}_history.csv\", index=False)\n",
    "\n",
    "    # Load best and evaluate on test\n",
    "    model.load_state_dict(torch.load(OUT_DIR / f\"{name}_best.pt\", map_location=DEVICE))\n",
    "    yte_true, yte_pred = eval_loader(test_loader)\n",
    "    m_te = multi_horizon_metrics(yte_true, yte_pred)\n",
    "    print(f\"\\nBEST TEST ({name}): MAE={m_te['mae']:.4f} RMSE={m_te['rmse']:.4f} R2={m_te['r2']:.4f}\")\n",
    "\n",
    "    # Plot learning curves\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(hist[\"epoch\"], hist[\"train_mae\"], label=\"train_MAE\")\n",
    "    plt.plot(hist[\"epoch\"], hist[\"val_mae\"], label=\"val_MAE\")\n",
    "    plt.plot(hist[\"epoch\"], hist[\"train_rmse\"], label=\"train_RMSE\")\n",
    "    plt.plot(hist[\"epoch\"], hist[\"val_rmse\"], label=\"val_RMSE\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.title(f\"{name}: MAE/RMSE over epochs (multi-horizon avg)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT_DIR / f\"{name}_curves.png\", dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # R2 curve\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.plot(hist[\"epoch\"], hist[\"train_r2\"], label=\"train_R2\")\n",
    "    plt.plot(hist[\"epoch\"], hist[\"val_r2\"], label=\"val_R2\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"R2\")\n",
    "    plt.title(f\"{name}: R2 over epochs (multi-horizon avg)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT_DIR / f\"{name}_r2.png\", dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    return m_te\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Main: Load parquet and run two experiments\n",
    "# =========================\n",
    "df = pd.read_parquet(PARQ_PATH)\n",
    "\n",
    "# Identify timestamp column in your parquet (support common variants)\n",
    "if \"timestamp_dt\" in df.columns:\n",
    "    ts_col = \"timestamp_dt\"\n",
    "    df[ts_col] = pd.to_datetime(df[ts_col])\n",
    "elif \"timestamp\" in df.columns:\n",
    "    ts_col = \"timestamp\"\n",
    "    # try parse to datetime if it looks like datetime strings\n",
    "    if df[ts_col].dtype == \"object\":\n",
    "        try:\n",
    "            df[ts_col] = pd.to_datetime(df[ts_col])\n",
    "            ts_col = \"timestamp\"\n",
    "        except Exception:\n",
    "            pass\n",
    "else:\n",
    "    raise ValueError(\"No timestamp column found (expected timestamp or timestamp_dt).\")\n",
    "\n",
    "if \"rsu_id\" not in df.columns:\n",
    "    raise ValueError(\"No rsu_id column found in parquet.\")\n",
    "if \"rsu_load_kbps\" not in df.columns:\n",
    "    raise ValueError(\"No rsu_load_kbps column found in parquet (target).\")\n",
    "\n",
    "# Add time features\n",
    "df = add_time_features(df, ts_col)\n",
    "\n",
    "# Ensure rsu_id is string\n",
    "df[\"rsu_id\"] = df[\"rsu_id\"].astype(\"string\")\n",
    "\n",
    "# Sort for safety\n",
    "df = df.sort_values([\"rsu_id\", ts_col]).reset_index(drop=True)\n",
    "\n",
    "# Define candidate feature columns (exclude keys + target)\n",
    "key_cols = {\"rsu_id\", ts_col, \"rsu_load_kbps\"}\n",
    "all_cols = set(df.columns)\n",
    "\n",
    "# Always include past load as feature 0 (decoder init expects feature 0 = load)\n",
    "# Minimal features:\n",
    "time_feat_cols = [c for c in df.columns if c.endswith(\"_sin\") or c.endswith(\"_cos\") or c in [\"hour\", \"dow\", \"minute\", \"t_sin\", \"t_cos\"]]\n",
    "time_feat_cols = [c for c in time_feat_cols if c in df.columns]\n",
    "\n",
    "# Exp-1: only load + time features (NO network/comm aggregates)\n",
    "x_cols_minimal = [\"rsu_load_kbps\"] + time_feat_cols\n",
    "\n",
    "# Exp-2: load + time + all aggregated metrics (numeric only)\n",
    "candidate = [c for c in df.columns if c not in key_cols and c != \"rsu_id\"]\n",
    "# keep numeric columns only\n",
    "numeric_candidate = [c for c in candidate if pd.api.types.is_numeric_dtype(df[c])]\n",
    "# remove target if accidentally included\n",
    "numeric_candidate = [c for c in numeric_candidate if c != \"rsu_load_kbps\"]\n",
    "# ensure time features included too\n",
    "x_cols_full = [\"rsu_load_kbps\"] + [c for c in numeric_candidate if c != \"rsu_load_kbps\"]\n",
    "\n",
    "print(f\"Timestamp column: {ts_col}\")\n",
    "print(f\"Minimal X cols: {len(x_cols_minimal)} -> {x_cols_minimal[:10]}\")\n",
    "print(f\"Full X cols:    {len(x_cols_full)} (includes all numeric aggregates)\")\n",
    "\n",
    "# Run experiments\n",
    "m1 = run_experiment(\"seq2seq_minimal\", df, ts_col, \"rsu_id\", \"rsu_load_kbps\", x_cols_minimal)\n",
    "m2 = run_experiment(\"seq2seq_full\", df, ts_col, \"rsu_id\", \"rsu_load_kbps\", x_cols_full)\n",
    "\n",
    "print(\"\\nSummary (TEST best checkpoints)\")\n",
    "print(f\"seq2seq_minimal: MAE={m1['mae']:.4f} RMSE={m1['rmse']:.4f} R2={m1['r2']:.4f}\")\n",
    "print(f\"seq2seq_full:    MAE={m2['mae']:.4f} RMSE={m2['rmse']:.4f} R2={m2['r2']:.4f}\")\n",
    "print(f\"\\nSaved outputs under: {OUT_DIR.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a92715f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23885928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046af1ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
